{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "# Load the datasets\n",
        "try:\n",
        "    train_df = pd.read_csv('/content/drive/MyDrive/DATA/train_2kmZucJ.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Ensure train_2kmZucJ.csv is uploaded.\")\n",
        "    train_df = pd.DataFrame({\n",
        "        'id': range(3),\n",
        "        'label': [0, 1, 0],\n",
        "        'tweet': ['I love my new phone!', 'My laptop is so slow and buggy $&@*#', 'Just got the new headset, amazing quality.']\n",
        "    })\n",
        "\n",
        "# --- Re-using Preprocessing Steps ---\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s\\$&@\\*#]', '', text)\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 1]\n",
        "    return \" \".join(cleaned_tokens)\n",
        "\n",
        "train_df['cleaned_tweet'] = train_df['tweet'].apply(preprocess_text)\n",
        "\n",
        "# --- Define features (X) and target (y) and Split Data ---\n",
        "X = train_df['cleaned_tweet']\n",
        "y = train_df['label']\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "\n",
        "# --- Build and Train the SVM Pipeline ---\n",
        "svm_pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(ngram_range=(1, 2), max_features=10000)),\n",
        "    ('svc', SVC(kernel='linear', class_weight='balanced', random_state=42, probability=True))\n",
        "])\n",
        "\n",
        "print(\"Training the SVM model...\")\n",
        "svm_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# --- Evaluate the SVM Model ---\n",
        "y_pred_svm = svm_pipeline.predict(X_val)\n",
        "val_f1_svm = f1_score(y_val, y_pred_svm, average='weighted')\n",
        "\n",
        "print(\"\\nSVM Model Evaluation:\")\n",
        "print(f\"Validation Weighted F1-Score: {val_f1_svm:.4f}\\n\")\n",
        "print(\"Validation Classification Report:\")\n",
        "print(classification_report(y_val, y_pred_svm, target_names=['Not Negative (0)', 'Negative (1)']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnQPKxuzbAmw",
        "outputId": "44e2a7da-167a-41ea-be36-62dad5268354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the SVM model...\n",
            "\n",
            "SVM Model Evaluation:\n",
            "Validation Weighted F1-Score: 0.8854\n",
            "\n",
            "Validation Classification Report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "Not Negative (0)       0.95      0.89      0.92      1179\n",
            "    Negative (1)       0.73      0.86      0.79       405\n",
            "\n",
            "        accuracy                           0.88      1584\n",
            "       macro avg       0.84      0.87      0.85      1584\n",
            "    weighted avg       0.89      0.88      0.89      1584\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# --- Assuming 'svm_pipeline' is the trained pipeline from the previous step ---\n",
        "# If you don't have it in memory, you would need to re-run the training code.\n",
        "\n",
        "# Load and preprocess the test data\n",
        "try:\n",
        "    test_df = pd.read_csv('/content/drive/MyDrive/DATA/test_oJQbWVk.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Ensure test_oJQbWVk.csv is uploaded.\")\n",
        "    test_df = pd.DataFrame({\n",
        "        'id': range(2),\n",
        "        'tweet': ['This computer is a piece of junk.', 'The camera on this tablet is fantastic.']\n",
        "    })\n",
        "\n",
        "# Re-using the same preprocessing function\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s\\$&@\\*#]', '', text)\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 1]\n",
        "    return \" \".join(cleaned_tokens)\n",
        "\n",
        "test_df['cleaned_tweet'] = test_df['tweet'].apply(preprocess_text)\n",
        "\n",
        "\n",
        "# --- Predict on the test data using the trained SVM pipeline ---\n",
        "print(\"Making predictions on the test set with the SVM model...\")\n",
        "X_test = test_df['cleaned_tweet']\n",
        "test_predictions_svm = svm_pipeline.predict(X_test)\n",
        "\n",
        "\n",
        "# --- Create and save the submission file ---\n",
        "submission_df_svm = pd.DataFrame({\n",
        "    'id': test_df['id'],\n",
        "    'label': test_predictions_svm\n",
        "})\n",
        "\n",
        "submission_df_svm.to_csv('submission_svm.csv', index=False)\n",
        "\n",
        "print(\"\\nSubmission file 'submission_svm.csv' created successfully!\")\n",
        "print(\"This file is ready for submission to the competition.\")\n",
        "print(\"\\nFirst 5 rows of the new submission file:\")\n",
        "print(submission_df_svm.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwsltXp6buKm",
        "outputId": "3faee625-b21a-4412-dcb3-ca885d9299e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making predictions on the test set with the SVM model...\n",
            "\n",
            "Submission file 'submission_svm.csv' created successfully!\n",
            "This file is ready for submission to the competition.\n",
            "\n",
            "First 5 rows of the new submission file:\n",
            "     id  label\n",
            "0  7921      1\n",
            "1  7922      1\n",
            "2  7923      1\n",
            "3  7924      1\n",
            "4  7925      1\n"
          ]
        }
      ]
    }
  ]
}